{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to PyTorch-Transformers: An Incredible Library for State-of-the-Art NLP (with Python code).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdgVZx/LMQpiStgbU5pzAV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayush9753/LearningResources/blob/master/Introduction_to_PyTorch_Transformers_An_Incredible_Library_for_State_of_the_Art_NLP_(with_Python_code).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tRFfGulWuVj",
        "outputId": "f91eab84-7aab-419b-dff4-85ccfd84c487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 2.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 11.5MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/5a/7abee53ad5a686b2785de6e4d2cdad628662c4f34dbb520b3cb2a88c10e4/boto3-1.15.9-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 16.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 15.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.6.0+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.16.0)\n",
            "Collecting botocore<1.19.0,>=1.18.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/74/88e40850f9f698b280ff11b387313bea34560c61a61be5e1fcd60b790aa9/botocore-1.18.9-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 14.8MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.6.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.19.0,>=1.18.9->boto3->pytorch-transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=41c622c884ac8c1a21cf50be3d4e0b81a28973b3349bc8bbdd4367441402d72a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, jmespath, botocore, s3transfer, boto3, sentencepiece, pytorch-transformers\n",
            "Successfully installed boto3-1.15.9 botocore-1.18.9 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFftvrWXbLen",
        "outputId": "37dda12e-aa89-41ee-a2ff-bc635503e432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#imprting required libraries\n",
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer , GPT2LMHeadModel\n",
        "# Load pre-trained model tokenizer (vocablury)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "#Load pre-trained model (weights) \n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "#Set the model to evaluation mode to deactivate Dropout modules\n",
        "model.eval()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1042301/1042301 [00:01<00:00, 932505.24B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 620005.26B/s]\n",
            "100%|██████████| 665/665 [00:00<00:00, 174478.43B/s]\n",
            "100%|██████████| 548118077/548118077 [00:34<00:00, 16087892.22B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh1s9DduggqH"
      },
      "source": [
        "def Get_the_word(model,tokenizer,text,i):\n",
        "  #Encode a text input\n",
        "  indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "  #Convert indexed tokens into PyTorch Tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  #If you have GPU , put everything on cuda\n",
        "  #tokens_tensor = tokens_tensor.to('cuda')\n",
        "  #model.to('cuda')\n",
        "  # Not including above three lines as I am working on google colab with GPU runtime type\n",
        "  #Predict all the tokens\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "    #Get the predicted next sub-word\n",
        "    predicted_index = torch.argmax(predictions[0,i,:]).item()\n",
        "    #prediction have a shape of [1,7,50257] where 7 is for np. of words in the sentence\n",
        "    #since we are searching for a word after last word we do -1\n",
        "    predicted_word = tokenizer.decode([predicted_index])\n",
        "    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "\n",
        "    #Printing the word \n",
        "    #print(predicted_word)\n",
        "    #print(predicted_text)\n",
        "    return predicted_text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04TI712bbcGq",
        "outputId": "f0a0b432-6493-4a51-a705-37c3cb526302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "line = 'Neeraj is a former member of the Indian'\n",
        "Get_the_word(model , tokenizer , line , i =-1)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Neeraj is a former member of the Indian Parliament'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "206JVDpLh_nC",
        "outputId": "dde01975-93e2-4f8f-ae2f-0d38a5236dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "line[-1]"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNqVz-Yljxp5",
        "outputId": "d2841551-e31a-4bb7-c086-ffe6bc26baf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "line = 'Neeraj is'\n",
        "while (line[-1] != '.'):\n",
        "  k = Get_the_word(model , tokenizer , line , i =-1)\n",
        "  line = k\n",
        "  print(line)\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Neeraj is a\n",
            " Neeraj is a former\n",
            " Neeraj is a former member\n",
            " Neeraj is a former member of\n",
            " Neeraj is a former member of the\n",
            " Neeraj is a former member of the Indian\n",
            " Neeraj is a former member of the Indian Parliament\n",
            " Neeraj is a former member of the Indian Parliament and\n",
            " Neeraj is a former member of the Indian Parliament and a\n",
            " Neeraj is a former member of the Indian Parliament and a member\n",
            " Neeraj is a former member of the Indian Parliament and a member of\n",
            " Neeraj is a former member of the Indian Parliament and a member of the\n",
            " Neeraj is a former member of the Indian Parliament and a member of the Indian\n",
            " Neeraj is a former member of the Indian Parliament and a member of the Indian Parliament\n",
            " Neeraj is a former member of the Indian Parliament and a member of the Indian Parliament's\n",
            " Neeraj is a former member of the Indian Parliament and a member of the Indian Parliament's Committee\n",
            " Neeraj is a former member of the Indian Parliament and a member of the Indian Parliament's Committee on\n",
            " Neeraj is a former member of the Indian Parliament and a member of the Indian Parliament's Committee on Foreign\n",
            " Neeraj is a former member of the Indian Parliament and a member of the Indian Parliament's Committee on Foreign Affairs\n",
            " Neeraj is a former member of the Indian Parliament and a member of the Indian Parliament's Committee on Foreign Affairs.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cjWv216kMMn",
        "outputId": "d67253b8-6910-4ebc-d1db-9fcda5ce9e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "line = 'Paro is'\n",
        "while (line[-1] != '.'):\n",
        "  k = Get_the_word(model , tokenizer , line , i =-1)\n",
        "  line = k\n",
        "  print(line)\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Paro is a\n",
            " Paro is a former\n",
            " Paro is a former member\n",
            " Paro is a former member of\n",
            " Paro is a former member of the\n",
            " Paro is a former member of the U\n",
            " Paro is a former member of the U.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43AoQyHYk0G8",
        "outputId": "a25d07ad-0129-446b-9c51-0a0bf61d54e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "line = 'Hrishikesh is'\n",
        "while (line[-1] != '.'):\n",
        "  k = Get_the_word(model , tokenizer , line , i =-1)\n",
        "  line = k\n",
        "  print(line)\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Hrishikesh is a\n",
            " Hrishikesh is a former\n",
            " Hrishikesh is a former chief\n",
            " Hrishikesh is a former chief minister\n",
            " Hrishikesh is a former chief minister of\n",
            " Hrishikesh is a former chief minister of Gujarat\n",
            " Hrishikesh is a former chief minister of Gujarat.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbxW9AOLlBSG",
        "outputId": "e69bd548-5a51-4128-c08d-5c1fa1b2b821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "line = 'Aayush Sharma is'\n",
        "while (line[-1] != '.'):\n",
        "  k = Get_the_word(model , tokenizer , line , i =-1)\n",
        "  line = k\n",
        "  print(line)\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Aayush Sharma is a\n",
            " Aayush Sharma is a senior\n",
            " Aayush Sharma is a senior editor\n",
            " Aayush Sharma is a senior editor at\n",
            " Aayush Sharma is a senior editor at The\n",
            " Aayush Sharma is a senior editor at The Hindu\n",
            " Aayush Sharma is a senior editor at The Hindu.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2alIfUJRlVmM",
        "outputId": "662ff9c1-a474-4849-ea87-4b06b520301d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "line = 'The pigeon'\n",
        "while (line[-1] != '.'):\n",
        "  k = Get_the_word(model , tokenizer , line , i =-1)\n",
        "  line = k\n",
        "  print(line)\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The pigeon is\n",
            " The pigeon is a\n",
            " The pigeon is a very\n",
            " The pigeon is a very small\n",
            " The pigeon is a very small bird\n",
            " The pigeon is a very small bird,\n",
            " The pigeon is a very small bird, but\n",
            " The pigeon is a very small bird, but it\n",
            " The pigeon is a very small bird, but it is\n",
            " The pigeon is a very small bird, but it is very\n",
            " The pigeon is a very small bird, but it is very intelligent\n",
            " The pigeon is a very small bird, but it is very intelligent.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRCwd1gLl_ut",
        "outputId": "699df23b-4005-4b47-a660-222bcd5a0ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "line = 'When that I was and a little tiny boy'\n",
        "while (line[-1] != '.'):\n",
        "  k = Get_the_word(model , tokenizer , line , i =-1)\n",
        "  line = k\n",
        "  print(line)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " When that I was and a little tiny boy,\n",
            " When that I was and a little tiny boy, I\n",
            " When that I was and a little tiny boy, I was\n",
            " When that I was and a little tiny boy, I was a\n",
            " When that I was and a little tiny boy, I was a little\n",
            " When that I was and a little tiny boy, I was a little bit\n",
            " When that I was and a little tiny boy, I was a little bit of\n",
            " When that I was and a little tiny boy, I was a little bit of a\n",
            " When that I was and a little tiny boy, I was a little bit of a boy\n",
            " When that I was and a little tiny boy, I was a little bit of a boy.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR1WbFBLnDaZ",
        "outputId": "a6faa515-4f66-41e2-f6e6-eca2f016da64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "line = 'IITs are very'\n",
        "while (line[-1] != '.'):\n",
        "  k = Get_the_word(model , tokenizer , line , i =-1)\n",
        "  line = k\n",
        "  print(line)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " IITs are very important\n",
            " IITs are very important to\n",
            " IITs are very important to the\n",
            " IITs are very important to the country\n",
            " IITs are very important to the country.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAqMW09To5ud",
        "outputId": "5c5f26e2-9370-4c48-955c-f355a48dd7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "line = 'Doctors are mostly'\n",
        "while (line[-1] != '.'):\n",
        "  k = Get_the_word(model , tokenizer , line , i =-1)\n",
        "  line = k\n",
        "  print(line)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Doctors are mostly in\n",
            " Doctors are mostly in the\n",
            " Doctors are mostly in the hospital\n",
            " Doctors are mostly in the hospital,\n",
            " Doctors are mostly in the hospital, but\n",
            " Doctors are mostly in the hospital, but they\n",
            " Doctors are mostly in the hospital, but they are\n",
            " Doctors are mostly in the hospital, but they are also\n",
            " Doctors are mostly in the hospital, but they are also in\n",
            " Doctors are mostly in the hospital, but they are also in the\n",
            " Doctors are mostly in the hospital, but they are also in the hospital\n",
            " Doctors are mostly in the hospital, but they are also in the hospital for\n",
            " Doctors are mostly in the hospital, but they are also in the hospital for a\n",
            " Doctors are mostly in the hospital, but they are also in the hospital for a number\n",
            " Doctors are mostly in the hospital, but they are also in the hospital for a number of\n",
            " Doctors are mostly in the hospital, but they are also in the hospital for a number of other\n",
            " Doctors are mostly in the hospital, but they are also in the hospital for a number of other conditions\n",
            " Doctors are mostly in the hospital, but they are also in the hospital for a number of other conditions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8GbMcNNl5bO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZasLR5G1raEC",
        "outputId": "87a63cd3-4728-4c1e-c4d9-713a8453ba88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/pytorch-transformers.git"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-transformers'...\n",
            "remote: Enumerating objects: 44599, done.\u001b[K\n",
            "remote: Total 44599 (delta 0), reused 0 (delta 0), pack-reused 44599\u001b[K\n",
            "Receiving objects: 100% (44599/44599), 32.02 MiB | 11.54 MiB/s, done.\n",
            "Resolving deltas: 100% (30950/30950), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbderZ2yr6BX",
        "outputId": "eabdd073-e97b-47c2-8ecd-7bc8fdf04124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!python pytorch-transformers/examples/\n",
        "#run_generation.py \\\n",
        "  #  --model_type=gpt2 \\\n",
        "   # --length=100 \\\n",
        "    #--model_name_or_path=gpt2 \\"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/bin/python3: can't find '__main__' module in 'pytorch-transformers/examples/'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us4VqAeKtBv4"
      },
      "source": [
        "#Training a Masked Language Model for BERT\n",
        "The BERT framework, a new language representation model from Google AI, uses pre-training and fine-tuning to create state-of-the-art NLP models for a wide range of tasks. These tasks include question answering systems, sentiment analysis, and language inference.\n",
        "\n",
        "BERT is pre-trained using the following two unsupervised prediction tasks:\n",
        "1.   Masked Language Modeling (MLM)\n",
        "2.   Next Sentence Prediction\n",
        "And you can implement both of these using PyTorch-Transformers. In fact, you can build your own BERT model from scratch or fine-tune a pre-trained version. So, let’s see how can we implement the Masked Language Model for BERT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yWQTqumz7UM"
      },
      "source": [
        "###PROBLEM STATEMENT\n",
        "Given an input sequence, we will randomly mask some words. The model then should predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUOEmf-Czlg2"
      },
      "source": [
        "import torch\n",
        "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "#Load pre-trained model tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Tokenize input\n",
        "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
        "tokenized_text = tokenizer.tokenize(text)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyWSBBNzrDyu",
        "outputId": "a0d93774-fad0-44f6-f2f8-e2fbb036081e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "tokenized_text"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'who',\n",
              " 'was',\n",
              " 'jim',\n",
              " 'henson',\n",
              " '?',\n",
              " '[SEP]',\n",
              " 'jim',\n",
              " 'henson',\n",
              " 'was',\n",
              " 'a',\n",
              " 'puppet',\n",
              " '##eer',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0WBnXvarqep"
      },
      "source": [
        "#Mask a token that we will try to predict with our 'BertForMaskedLM'\n",
        "masked_index = 8 #index of the word that we will index\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "# assert is used to check a statement , if it does not pass the checkpoint, it will give error\n",
        "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
        "\n",
        "#Convert tokens tovocablury indices\n",
        "indexed_token = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "#Define A and B indices associated with 1st and 2nd sentences (See paper)\n",
        "segments_ids = [0,0,0,0,0,0,0,1,1,1,1,1,1,1]\n",
        "\n",
        "#Convert inputs to PyTorch Tensors\n",
        "tokens_tensor = torch.tensor([indexed_token])\n",
        "segments_tensor = torch.tensor([segments_ids])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoS4C6ZXxoF0"
      },
      "source": [
        "Notice that **we have set [MASK] at the 8th index in the sentence which is the word ‘Henson’**. This is what our model will try to predict.\n",
        "\n",
        "Now that our data is rightly pre-processed for BERT, we will create a Masked Language Model. Let’s now use BertForMaskedLM to predict a masked token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDokOinIwKP1",
        "outputId": "47c76ff0-d83b-4b7d-a34c-1dfa873e22a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#LOad pre-trained model (weights)\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.eval()\n",
        "\n",
        "#If you have a GPU put everthing into CUDA\n",
        "#tokens_tensor = tokens_tensor.to('cuda')\n",
        "#segments_tensors = segments_tensors.to('cuda')\n",
        "#model.to('cuda')\n",
        "'''I am using GOOGLE COLAB in GPU runtime'''\n",
        "\n",
        "#Predict all the tokens\n",
        "with torch.no_grad():\n",
        "  outputs = model(tokens_tensor, token_type_ids = segments_tensor)\n",
        "  predictions = outputs[0]\n",
        "\n",
        "#Confirm we were able to predict 'henson'\n",
        "predicted_index = torch.argmax(predictions[0,masked_index]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]  #convert_ids_to_tokens gives alist for the masked word, so we access it via [0]\n",
        "assert predicted_token=='henson'\n",
        "print('Predicted token is ',predicted_token)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted token is  henson\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMW5DJfH0_x7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}