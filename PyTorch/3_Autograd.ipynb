{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3_Autograd.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN8y1E4VCZLBuhwOZTwX8be"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"GkdQQGxjjm_c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1600254384080,"user_tz":-330,"elapsed":1329,"user":{"displayName":"Aayush Sharma","photoUrl":"","userId":"06110750062533504361"}},"outputId":"cb1c0bfe-e86b-4918-ffdb-ff7980b0d8c7"},"source":["import torch\n","# The autograd package provides automatic differentiation \n","# for all operations on Tensors\n","x = torch.randn(3,requires_grad=True)\n","# requires_grad = True -> tracks all operations on the tensor. \n","y = x+2\n","\n","# y was created as a result of an operation, so it has a grad_fn attribute.\n","# grad_fn: references a Function that has created the Tensor\n","print(x) # created by the user -> grad_fn is None\n","print(y)\n","print(y.grad_fn)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["tensor([ 0.0841, -0.4883, -0.4887], requires_grad=True)\n","tensor([2.0841, 1.5117, 1.5113], grad_fn=<AddBackward0>)\n","<AddBackward0 object at 0x7f22c87eb358>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bza1QgHJkeIy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1600254695226,"user_tz":-330,"elapsed":1357,"user":{"displayName":"Aayush Sharma","photoUrl":"","userId":"06110750062533504361"}},"outputId":"01a24bdf-db95-4d58-ec95-17621c699896"},"source":["# Do more operations on y\n","z = y * y * 3\n","print(z)\n","z = z.mean()\n","print(z)\n","z = z/2\n","print(z)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["tensor([13.0299,  6.8560,  6.8525], grad_fn=<MulBackward0>)\n","tensor(8.9128, grad_fn=<MeanBackward0>)\n","tensor(4.4564, grad_fn=<DivBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S_YXzkphkePv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600254695228,"user_tz":-330,"elapsed":929,"user":{"displayName":"Aayush Sharma","photoUrl":"","userId":"06110750062533504361"}},"outputId":"ed709f4e-d7f0-4623-a584-62ffcb230de9"},"source":["# Let's compute the gradients with backpropagation\n","# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n","# The gradient for this tensor will be accumulated into .grad attribute.\n","# It is the partial derivate of the function w.r.t. the tensor\n","\n","z.backward()\n","print(x.grad) # dz/dx"],"execution_count":33,"outputs":[{"output_type":"stream","text":["tensor([10.4203,  7.5586,  7.5567])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j52TiULxkeiA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1600254896982,"user_tz":-330,"elapsed":1202,"user":{"displayName":"Aayush Sharma","photoUrl":"","userId":"06110750062533504361"}},"outputId":"1c400f09-eb7b-4788-acb4-b65496829e26"},"source":["a = torch.randn(6,requires_grad=True)\n","b = torch.randn(6,requires_grad=True)\n","c = a + b\n","print(c)\n","c = c.mean()\n","print(a)\n","print(b)\n","print(c) \n","c.backward()\n","print(a.grad)\n","print(b.grad)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["tensor([-0.5306, -2.1249, -0.3306,  0.6597, -1.5669, -1.8413],\n","       grad_fn=<AddBackward0>)\n","tensor([-2.2768, -1.6085, -0.7734,  0.4668, -0.4574, -1.6058],\n","       requires_grad=True)\n","tensor([ 1.7462, -0.5164,  0.4428,  0.1929, -1.1096, -0.2355],\n","       requires_grad=True)\n","tensor(-0.9558, grad_fn=<MeanBackward0>)\n","tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n","tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YC8CkcB7ke74","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1600255631736,"user_tz":-330,"elapsed":1114,"user":{"displayName":"Aayush Sharma","photoUrl":"","userId":"06110750062533504361"}},"outputId":"5e7bbbce-937f-4813-c60b-08c3d5deeea2"},"source":["# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n","# It computes partial derivates while applying the chain rule\n","\n","# -------------\n","# Model with non-scalar output:\n","# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \n","# specify a gradient argument that is a tensor of matching shape.\n","# needed for vector-Jacobian product\n","\n","x = torch.randn(3, requires_grad=True)\n","y = x * 2\n","\n","for _ in range(10):\n","    y = y * 2\n","print(y)\n","print(y.shape)\n","\n","v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n","y.backward(v)\n","print(x.grad)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["tensor([ -206.7797, -2248.5393,  1711.2466], grad_fn=<MulBackward0>)\n","torch.Size([3])\n","tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R0yiA_GTkfDy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1600256101710,"user_tz":-330,"elapsed":1112,"user":{"displayName":"Aayush Sharma","photoUrl":"","userId":"06110750062533504361"}},"outputId":"c544f049-c916-4521-9290-4ebf81c42f04"},"source":["# -------------\n","# Stop a tensor from tracking history:\n","# For example during our training loop when we want to update our weights\n","# then this update operation should not be part of the gradient computation\n","# - x.requires_grad_(False)\n","# - x.detach()\n","# - wrap in 'with torch.no_grad():'\n","\n","# .requires_grad_(...) changes an existing flag in-place.\n","a = torch.randn(2, 2)\n","print(a.requires_grad)\n","b = ((a * 3) / (a - 1))\n","print(b.grad_fn)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b.grad_fn)\n","b.backward()\n","print(a.grad)"],"execution_count":70,"outputs":[{"output_type":"stream","text":["False\n","None\n","True\n","<SumBackward0 object at 0x7f2275ae2278>\n","tensor([[ 1.6903,  1.6002],\n","        [-0.1939, -1.8499]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1NGrMK52oTlZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600256380512,"user_tz":-330,"elapsed":1086,"user":{"displayName":"Aayush Sharma","photoUrl":"","userId":"06110750062533504361"}},"outputId":"5f4704e3-87d7-4430-ec74-a839d780bba5"},"source":["# .detach(): get a new Tensor with the same content but no gradient computation:\n","a = torch.randn(2, 2, requires_grad=True)\n","print(a.requires_grad)\n","b = a.detach()\n","print(b.requires_grad)"],"execution_count":85,"outputs":[{"output_type":"stream","text":["True\n","False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P4XC8NiloT30","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1600256639642,"user_tz":-330,"elapsed":1166,"user":{"displayName":"Aayush Sharma","photoUrl":"","userId":"06110750062533504361"}},"outputId":"e76900c2-068b-4466-f73b-02638c918e2d"},"source":["# wrap in 'with torch.no_grad():'\n","a = torch.randn(2, 2, requires_grad=True)\n","print(a.requires_grad)\n","with torch.no_grad():\n","    print((x ** 2).requires_grad)\n","    print(x)\n","    print((a*2).requires_grad)\n","    print(a.requires_grad)\n","print(a)"],"execution_count":97,"outputs":[{"output_type":"stream","text":["True\n","False\n","tensor([-0.3300], requires_grad=True)\n","False\n","True\n","tensor([[-1.6050, -0.0854],\n","        [-0.4412, -0.5626]], requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jpfSrJCjkfPn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1600256857019,"user_tz":-330,"elapsed":1147,"user":{"displayName":"Aayush Sharma","photoUrl":"","userId":"06110750062533504361"}},"outputId":"c8640720-4c0a-41d9-ee7d-d52337f58556"},"source":["# -------------\n","# backward() accumulates the gradient for this tensor into .grad attribute.\n","# !!! We need to be careful during optimization !!!\n","# Use .zero_() to empty the gradients before a new optimization step!\n","weights = torch.ones(4, requires_grad=True)\n","print(weights)\n","for epoch in range(3):\n","    # just a dummy example\n","    model_output = (weights*3).sum()\n","    model_output.backward()\n","    \n","    print(weights.grad)\n","\n","    # optimize model, i.e. adjust weights...\n","    with torch.no_grad():\n","        weights -= 0.1 * weights.grad\n","\n","    # this is important! It affects the final weights & output\n","    weights.grad.zero_()\n","\n","print(weights)\n","print(model_output)\n","\n","# Optimizer has zero_grad() method\n","# optimizer = torch.optim.SGD([weights], lr=0.1)\n","# During training:\n","# optimizer.step()\n","# optimizer.zero_grad()"],"execution_count":104,"outputs":[{"output_type":"stream","text":["tensor([1., 1., 1., 1.], requires_grad=True)\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n","tensor(4.8000, grad_fn=<SumBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Cm10yIjIu5qT","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}